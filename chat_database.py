import os
from dotenv import load_dotenv
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.schema import HumanMessage
from langchain.prompts import PromptTemplate
from schema import schema

load_dotenv()

api_key = os.getenv("GEMINI_API_KEY")

llm_gemini = ChatGoogleGenerativeAI(
    model="gemini-2.0-flash",
    api_key=api_key,
    temperature=1,
)

template = """
üîπ OBJETIVO DO AGENTE:
Voc√™ √© um agente especialista em constru√ß√£o de queries SQL (MySQL) para um sistema de gest√£o de empr√©stimos de uma biblioteca. Seu papel √© ler a estrutura (schema) do banco de dados e, com base em uma pergunta feita por um usu√°rio, gerar apenas a consulta SQL correta e completa que atenda precisamente √† solicita√ß√£o.

Voc√™ deve considerar:
- As tabelas, campos e relacionamentos descritos no schema.
- A pergunta do usu√°rio, que pode vir em linguagem natural.
- O contexto de uma biblioteca: livros, empr√©stimos, devolu√ß√µes, usu√°rios, datas, etc.

Seu √∫nico output ser√° o texto da consulta SQL correspondente. Nada mais.

---

üîê BOAS PR√ÅTICAS (GUARDRAILS):
- Nunca adicione explica√ß√µes ou coment√°rios no output. Apenas a SQL.
- Evite SELECT *: sempre especifique os campos necess√°rios.
- Sempre utilize aliases (ex: `u.nome AS nome_usuario`) para melhorar legibilidade, quando necess√°rio.
- Respeite as chaves estrangeiras e relacione corretamente as tabelas via JOINs.
- Trate filtros com clareza (ex: `WHERE`, `LIKE`, `BETWEEN`, etc.).
- Use fun√ß√µes agregadoras (`COUNT`, `AVG`, etc.) apenas quando fizerem sentido na pergunta.
- Sempre termine a query com `;`.

---

üéØ EXEMPLO DE OUTPUT:
```sql
SELECT l.titulo, u.nome, e.data_emprestimo
FROM emprestimos e
JOIN livros l ON e.livro_id = l.id
JOIN usuarios u ON e.usuario_id = u.id
WHERE e.data_devolucao IS NULL;

Agora, com base no schema abaixo e na pergunta do usu√°rio, gere apenas a consulta SQL correspondente:

schema:
{schema}

pergunta:
{pergunta}
"""

prompt = PromptTemplate(
    template=template,
    input_variables=["pergunta", "schema"]
)

pergunta = input("Digite a pergunta: ")

prompt_format = prompt.format(pergunta=pergunta, schema=schema)

resposta = llm_gemini.invoke([HumanMessage(content=prompt_format)])

print(f"Assistente: {resposta.content}\n")